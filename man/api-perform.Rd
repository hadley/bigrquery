% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bq-perform.R
\name{api-perform}
\alias{api-perform}
\alias{api-perform}
\alias{bq_perform_extract}
\alias{api-perform}
\alias{bq_perform_upload}
\alias{bq_perform_query}
\alias{bq_perform_copy}
\title{BigQuery jobs: perform a job}
\usage{
bq_perform_extract(x, destination_uris,
  destination_format = "NEWLINE_DELIMITED_JSON", compression = "NONE", ...,
  print_header = TRUE, billing = x$project)

bq_perform_upload(x, values, create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_APPEND", ..., billing = x$project)

bq_perform_query(query, billing, ..., destination_table = NULL,
  default_dataset = NULL, create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_EMPTY", use_legacy_sql = FALSE)

bq_perform_copy(src, dest, create_disposition = "CREATE_IF_NEEDED",
  write_disposition = "WRITE_EMPTY", ..., billing = NULL)
}
\arguments{
\item{x}{A \link{bq_table}}

\item{destination_uris}{A character vector of fully-qualified Google Cloud
Storage URIs where the extracted table should be written. Can export
up to 1 Gb of data per file. Use a wild card URI (e.g.
\code{gs://[YOUR_BUCKET]/file-name-*.json}) to automatically create any
number of files.}

\item{destination_format}{The exported file format. Possible values
include "CSV", "NEWLINE_DELIMITED_JSON" and "AVRO". Tables with nested or
repeated fields cannot be exported as CSV.}

\item{compression}{The compression type to use for exported files. Possible
values include "GZIP", "DEFLATE", "SNAPPY", and "NONE". "DEFLATE" and
"SNAPPY" are only supported for Avro.}

\item{...}{Additional arguments passed on to the underlying API call.
snake_case names are automatically converted to camelCase.}

\item{print_header}{Whether to print out a header row in the results.}

\item{billing}{Identifier of project to bill.}

\item{values}{Data frame of values to insert.}

\item{create_disposition}{Specifies whether the job is allowed to create
new tables.

The following values are supported:
\itemize{
\item "CREATE_IF_NEEDED": If the table does not exist, BigQuery creates the
table.
\item "CREATE_NEVER": The table must already exist. If it does not, a
'notFound' error is returned in the job result.
}}

\item{query}{SQL query string.}

\item{destination_table}{A \link{bq_table} where results should be stored.
If not supplied, results will be saved to a temporary table that lives
in a special dataset. You must supply this parameter for large
queries (> 128 MB compressed).}

\item{default_dataset}{A \link{bq_dataset} used to automatically qualify table names.}

\item{use_legacy_sql}{If \code{TRUE} will use BigQuery's legacy SQL format.}

\item{write_dispoition}{Specifies the action that occurs if the
destination table already exists. The following values are supported:
\itemize{
\item "WRITE_TRUNCATE": If the table already exists, BigQuery overwrites the
table data.
\item "WRITE_APPEND": If the table already exists, BigQuery appends the data
to the table.
\item "WRITE_EMPTY": If the table already exists and contains data, a
'duplicate' error is returned in the job result.
}}
}
\value{
A \link{bq_job}.
}
\description{
These functions are low-level functions designed to be used by experts.
Each of these low-level functions is paired with a high-level function that
you should use instead:
\itemize{
\item \code{bq_perform_copy()}:    \code{\link[=bq_table_copy]{bq_table_copy()}}.
\item \code{bq_perform_extract()}: \code{\link[=bq_table_extract]{bq_table_extract()}}.
\item \code{bq_perform_query()}:   \code{\link[=bq_dataset_query]{bq_dataset_query()}}, \code{\link[=bq_project_query]{bq_project_query()}}.
\item \code{bq_perform_upload()}:  \code{\link[=bq_table_upload]{bq_table_upload()}}.
}
}
\section{API documentation}{

\itemize{
\item \href{https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs}{jobs}
}

Additional information at:
\itemize{
\item \href{https://cloud.google.com/bigquery/docs/exporting-data}{exporting data}
\item \href{https://cloud.google.com/bigquery/docs/loading-data}{loading data}
\item \href{https://cloud.google.com/bigquery/docs/writing-results}{writing queries}
\item \href{https://cloud.google.com/bigquery/docs/managing-tables#copy-table}{copying a table}
}
}

\examples{
if (bq_testable()) {
ds <- bq_test_dataset()
bq_mtcars <- bq_table(ds, "mtcars")
job <- bq_perform_upload(bq_mtcars, mtcars)
bq_table_exists(bq_mtcars)

bq_job_wait(job)
bq_table_exists(bq_mtcars)
head(bq_table_download(bq_mtcars))
}
}
\keyword{internal}
